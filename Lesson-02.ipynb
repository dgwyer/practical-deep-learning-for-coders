{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efa3e24-4049-4b35-a418-e5190c20dcf5",
   "metadata": {},
   "source": [
    "# Install dependencies and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db902547-2287-4aaf-a943-8f2c5610fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq fastbook fastai duckduckgo_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6349749-6724-4046-a694-124a193b8ff4",
   "metadata": {},
   "source": [
    "I quite like how you can see which libraries are currently installed. Here is a list of all libraries currently available that begin with 'f'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19377e36-3ec5-4010-9bd8-20b39c857f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai                       2.7.13\n",
      "fastbook                     0.0.29\n",
      "fastcore                     1.5.29\n",
      "fastdownload                 0.0.7\n",
      "fastjsonschema               2.16.2\n",
      "fastprogress                 1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep \"^fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a59090-37c7-4abf-855d-d34a5fe698de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "from duckduckgo_search import DDGS\n",
    "from fastcore.all import *\n",
    "from itertools import islice\n",
    "from IPython.display import HTML\n",
    "from shutil import move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49adb91-b8c7-4c92-b0f9-2a5100b208d5",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a48409ff-b2ea-4cd6-92b2-5213a16c8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for images and return a list of URLs.\n",
    "def search_images(term, max_images = 250):\n",
    "    print(f\"Searching for {max_images} '{term}' images...\")\n",
    "    keywords = term\n",
    "    ddgs_images = ddgs.images(keywords, max_results=max_images)\n",
    "    #limited_images = list(islice(ddgs_images, max_images))\n",
    "    return L(ddgs_images).itemgot('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73eb87c5-5e90-4325-9adf-661d56f27887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images for the specifed search term.\n",
    "#\n",
    "# 1. If the search term contains spaces then hyphenate for the folder name.\n",
    "# 2. Skip if images already exist.\n",
    "#\n",
    "def download_image_urls(search_term, max_images, resize=True):\n",
    "    search = search_images(search_term, max_images)\n",
    "    dest_search_term = search_term.replace(' ', '-')\n",
    "    dest = Path(path)/dest_search_term\n",
    "    if dest.exists() and any(dest.iterdir()):\n",
    "        num_images_before = len([1 for _ in dest.iterdir()])\n",
    "        print(f\"{num_images_before} images already downloaded in {dest}, skipping download.\")\n",
    "    else:\n",
    "        dest.mkdir(exist_ok=True, parents=True)\n",
    "        download_images(dest, urls=search)\n",
    "        num_images_downloaded = len([1 for _ in dest.iterdir()])\n",
    "        if resize:\n",
    "            print(f\"Resizing images...\")\n",
    "            resize_images(dest, max_size=400, dest=dest)\n",
    "        print(f\"{num_images_downloaded} images downloaded and resized in {dest}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "799024d7-cc3a-4c87-8f5c-27c9c4386e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify downloaded files.\n",
    "#\n",
    "# 1. Move failed images to '/deleted' folder for inspection.\n",
    "# 2. Output information on number of files checked, and failed images.\n",
    "#\n",
    "def verify_downloaded_images():\n",
    "    image_files = get_image_files(path)\n",
    "    num_images_checked = len(image_files)\n",
    "    print(f\"Number of images to be checked: {num_images_checked}\")\n",
    "\n",
    "    failed = verify_images(image_files)\n",
    "\n",
    "    # Create a 'deleted' folder if it doesn't exist\n",
    "    deleted_folder = Path(path).parent / 'deleted'\n",
    "    deleted_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    for failed_image in failed:\n",
    "        new_location = deleted_folder / failed_image.name\n",
    "        print(f\"Moving failed image: {failed_image} to {new_location}\")\n",
    "        move(failed_image, new_location)\n",
    "        \n",
    "    print(f\"Number of failed images moved to 'deleted' folder: {len(failed)}\")\n",
    "    return failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d25ffbf-793e-4531-acb0-65f98614472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images used in the training set, and validation set.\n",
    "def displayRandomSplitter(v_pct=0.2, sd=42):\n",
    "    # Get a list of files (or items) in your dataset\n",
    "    items = get_image_files(path)\n",
    "    \n",
    "    # Initialize the RandomSplitter\n",
    "    splitter = RandomSplitter(valid_pct=v_pct, seed=sd)\n",
    "    \n",
    "    # Apply the splitter to your dataset\n",
    "    train_indices, valid_indices = splitter(items)\n",
    "    \n",
    "    # Function to create a DataFrame from file indices\n",
    "    def create_dataframe(file_indices):\n",
    "        data = [(items[i].name, items[i].parent.name) for i in file_indices]\n",
    "        df = pd.DataFrame(data, columns=['Filename', 'Parent Folder'])\n",
    "        #sorted = df.sort_values(by='Parent Folder')\n",
    "        return df\n",
    "    \n",
    "    # Create DataFrames for training and validation sets\n",
    "    train_files = create_dataframe(train_indices)\n",
    "    valid_files = create_dataframe(valid_indices)\n",
    "    \n",
    "    # Convert DataFrames to HTML\n",
    "    train_html = train_files.to_html(index=False)\n",
    "    valid_html = valid_files.to_html(index=False)\n",
    "    \n",
    "    # Display the HTML tables side by side\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"float: left; padding-right: 20px;\">\n",
    "        <h3 style=\"text-align:center\">Training Set Files</h3>\n",
    "        {train_html}\n",
    "    </div>\n",
    "    <div style=\"float: left;padding-left:50px;\">\n",
    "        <h3 style=\"text-align:center\">Validation Set Files</h3>\n",
    "        {valid_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0f626-b41f-413d-8ad8-7c9699647018",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Notes from Lesson 2 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df86a5-a02f-4759-aaa2-0d9ce52b74d1",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Important to understand that `RandomResizedCrop` will only generate **one** cropped (and resized) image for each image in the training set, for each epoch. This does not include the original unaltered image at all apparently.\n",
    "\n",
    "My understanding is that if you have `RandomResizedCrop` enabled, for each epoch it will take each image in the training set and randomly crop and resize it. And at no time is the original unaltered image used in any epochs.\n",
    "\n",
    "I found that quite interesting as I initially thought that `RandomResizedCrop` randomly generated a bunch of images based on the original image for each epoch, so this clarification is really useful.\n",
    "\n",
    "At some point in the future I wonder what effect on a model, using the original unaltered image for the first epoch would have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8fa28-72ed-4a27-9389-ab330f1195e6",
   "metadata": {},
   "source": [
    "## Using PKL files on Hugging Face\n",
    "\n",
    "I think there are some general [security concerns](https://forums.fast.ai/t/lesson-2-official-topic/96033/710) when using `*.pkl` files. And Hugging Face will usually display a warning when you try to use them.\n",
    "\n",
    "[Safetensors](https://huggingface.co/docs/safetensors/index) seem to be the preferred alternative to `*.pkl` files thses days.\n",
    "\n",
    "It will be interesting to see if fastai adds support for the Safetensors format in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301eaf31-709d-4f7c-9018-970c1df5713c",
   "metadata": {},
   "source": [
    "## Exporting code from a notebook\n",
    "\n",
    "Just over halfway through Lesson 2 in the video lecture Jeremy discusses how to [author code](https://youtu.be/F4tvM4Vb3A0?t=2905) in a notebook and export it for use in other applications, such as a Gradio application hosted on Hugging Face.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca889a-e47b-4ec9-9bd3-633f533359ce",
   "metadata": {},
   "source": [
    "## Using a PKL model in JavaScript application\n",
    "\n",
    "Jeremy discusses how to use a trained model exported as a `*.pkl` file in a JavaScript application by hosting the model on a server (e.g. Hugging Face) and using the api to make predictions on the model and return results. This is OK for testing but for production you would probably want to host your model on a dedicated server platform such as Vercel or one of the many others available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631c61b-29db-4c20-9715-3ff999bae81b",
   "metadata": {},
   "source": [
    "# Homework\n",
    "My own classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05cf13-fa0a-4625-b3ae-0438e261ec0f",
   "metadata": {},
   "source": [
    "Needed to do a bit of manual data cleaning. There were quite a few cartoon type images that I removed, as well as anything that didn't look like it belonged in the data set.\n",
    "\n",
    "In Lesson 2 Jeremy recommends training the model before doing any data cleaning, but in this notebook I cleaned the data manually before doing any training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb3ad1-2088-4af7-a70e-d145d1fac288",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I went through the lesson one video of the course and followed along with the bird classifier, and modified a bit of the code to further my knowledge and experience of Python. Overall I was quite happy with my understanding of most of the topics presented.\n",
    "\n",
    "I think my biggest takeaway is a desire to understand more about the `fine_tune()` method, and related methods such as `fit_one_cycle()` at a fundamental level. Plus all the `DataBlock` and `DataLoaders` methods/classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27316c2-425a-495a-be65-9305b75d76ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
